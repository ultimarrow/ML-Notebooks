{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a2977c",
   "metadata": {},
   "source": [
    "# COMP5318 Week 11: Hidden Markov Models\n",
    "\n",
    "Hidden Markov Models (HMMs) are a class of probabilistic models which allow us to relate observed sequences of events to hidden events which we consider as causal factors. \n",
    "\n",
    "While useful in many spheres, HMMs and related probabilistic sequence models such as Conditional Random Fields (CRFs) have found widespread use in Natual Language Processing. Many NLP sequence labelling problems can be fit into this framework by considering words as observations and searching for a sequence of hidden tags, such as part-of-speech labels (noun, verb etc.). As elaborated at the end of today's tutorial, HMMs also have applications in bioinformatics.\n",
    "\n",
    "Today, we aim to strengthen your understanding of the key algorithms underlying HMMs by implementing them and examining their use on some problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f99d06c",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "We begin by importing some packages used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77470991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# Make the notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bc80f5",
   "metadata": {},
   "source": [
    "## 2. Definitions\n",
    "\n",
    "Let's introduce our first problem, which was originally devised by [Jason Eisner (2002)](https://www.cs.jhu.edu/~jason/papers/eisner.tnlp02.pdf). \n",
    "\n",
    "> \"You are climatologists in the year 2799, studying the history of global warming. You can’t find any records of Baltimore weather, but you do find my diary, in which I assiduously recorded how much ice cream I ate each day. What can you figure out from this about the weather that summer?\"\n",
    "\n",
    "In this case, we want to relate the observations of Jason's ice-cream consumption with the weather hidden states. To simplify, suppose the days were either Hot or Cold, and Jason ate only 0, 1, or 2 ice creams each day.\n",
    "\n",
    "Before diving into our investigation, we need to recap the required components for an HMM.\n",
    "\n",
    "- A set of N possible hidden states: $\\pi=\\pi_1, \\pi_2, \\dots, \\pi_N$\n",
    "\n",
    "- A sequence of M observations: $O = o_1 o_2 \\dots o_M$\n",
    "\n",
    "- A transition probability matrix, with probabilities $a_{ij}$ of moving from state i to state j: $A=a_{11} \\dots a_{ij} \\dots a_{NN}$\n",
    "\n",
    "- An initial probability distribution over the states: $A_0=a_1, a_2, \\dots, a_N$\n",
    "\n",
    "- An emission probability matrix, with conditional probabilities $P(o_j|\\pi_i)$ of the observations for each state:\n",
    "$E=e_{\\pi_{i}}(o_j)$\n",
    "\n",
    "Let's assume that it is possible to estimate the probability matrices from other sources of data such as weather trends and ice cream sales trends."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d4dae8",
   "metadata": {},
   "source": [
    "**Transition probabilities**\n",
    "\n",
    "Suppose we have some intuition or evidence that if today is cold, tomorrow is likely to be cold, while if today is hot, tomorrow is probably hot. A transitions matrix encoding that might be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e6e871",
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = np.array([[0.8, 0.2],\n",
    "                        [0.3, 0.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7a0d6",
   "metadata": {},
   "source": [
    "**Initial probabilities**\n",
    "\n",
    "We also might suppose that we have an equal chance of beginning on a relatively hot or cold day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "404780a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "initials = np.array([0.5, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773810a0",
   "metadata": {},
   "source": [
    "**Emission probabilities**\n",
    "\n",
    "On cold days, it seems that Jason doesn't generally like to eat ice cream except for rare occasions, but on hot days Jason often likes to eat 2 ice creams. Our emission probability matrix might look a bit like the following, where the rows correspond to observations {0, 1, 2}, and the columns correspond to states {Hot, Cold}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7912aa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions = np.array([[0.1, 0.7],\n",
    "                      [0.2, 0.2],\n",
    "                      [0.7, 0.1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b70b2",
   "metadata": {},
   "source": [
    "We can use the Pandas library to display these matrices in a more readable format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0841165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toHot</th>\n",
       "      <th>toCold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fromHot</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fromCold</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          toHot  toCold\n",
       "fromHot     0.8     0.2\n",
       "fromCold    0.3     0.7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Initial Prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hot</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cold</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Initial Prob\n",
       "Hot            0.5\n",
       "Cold           0.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hot</th>\n",
       "      <th>Cold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hot  Cold\n",
       "0  0.1   0.7\n",
       "1  0.2   0.2\n",
       "2  0.7   0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Pandas dataframes from arrays\n",
    "df_transitions = pd.DataFrame(transitions, index=[\"fromHot\", \"fromCold\"], columns=[\"toHot\", \"toCold\"])\n",
    "df_initials = pd.DataFrame(initials, index=[\"Hot\", \"Cold\"], columns=[\"Initial Prob\"])\n",
    "df_emissions = pd.DataFrame(emissions, index=[\"0\", \"1\", \"2\"], columns=[\"Hot\", \"Cold\"])\n",
    "\n",
    "display(df_transitions)\n",
    "display(df_initials)\n",
    "display(df_emissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8b410",
   "metadata": {},
   "source": [
    "We've now specified an HMM which we can use to investigate our problem! You will remember from the lecture that HMMs are often specified as a graph to aid our interpretation of the model.\n",
    "\n",
    "**Question**\n",
    "\n",
    "Draw a graph illustrating this HMM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe04524f",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "See the way HMMs are drawn in the lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e69aa",
   "metadata": {},
   "source": [
    "## 3. Evaluation\n",
    "\n",
    "Recall that HMMs are characterised by three fundamental problems. The first is evaluation, otherwise known as likelihood computation.\n",
    "\n",
    "> Given an HMM and an observation sequence, determine the probability of the observed sequence.\n",
    "\n",
    "Recall from the lecture that this is easy to solve using the emission probabilites if we assume a particular set of hidden states. However, using this method we would have to consider all possible sequences of states which could generate the observations, which quickly becomes infeasible as the sequence length grows.\n",
    "\n",
    "Instead, we derived the forward algorithm, a dynamic programming algorithm which stores a table of intermediate values to make the computation of observation probabilities summed over all possible hidden state paths more efficient [$O(N^2M)$, using our notation from above].\n",
    "\n",
    "The intermediate values it keeps are the forward probabilities:\n",
    "\n",
    "$\\begin{aligned} f_{k}(i) &=P\\left(o_{1}, \\cdots, o_{i}, \\pi_{i}=k\\right) \\\\ &=e_{k}\\left(o_{i}\\right) \\sum_{j} f_{j}(i-1) a_{j k} \\end{aligned}$\n",
    "\n",
    "Recall the key steps of the algorithm from the lecture:\n",
    "\n",
    "*Initialisation*: $f_{k}(0) = a_k * e_{\\pi_{k}}(o_0)$ \n",
    "\n",
    "*Iteration*: $f_{k}(i)=\\sum_{j} f_{j}(i-1) * a_{j k} * e_{k}\\left(x_{i}\\right) $\n",
    "\n",
    "*Termination*: $P(o_1 o_2 \\dots o_M)=\\sum_{k} f_{k}(n)$\n",
    "\n",
    "Let's implement the forward algorithm using numpy. Rather than explicitly do these calculations for each state, we use matrix multiplication to handle all the states at once. Carefully go through the implementation and see if you can find each step of the algorithm in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9660234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(obs_seq, transitions, emissions, initials, verbose=False):\n",
    "    \"\"\"Given HMM parameters, find the probability of an observation sequence.\n",
    "    \n",
    "    Args:\n",
    "      obs_seq: 1D numpy array containing a sequence of observations encoded as indices\n",
    "      transitions: 2D square numpy array of transition probabilities between states\n",
    "      emissions: 2D numpy array of shape (num_obs, num_states) containing emissions probabilities P(obs|state)\n",
    "      initials: 1D numpy array of initial distribution over states\n",
    "      verbose: If True, print output during computation\n",
    "      \n",
    "    Returns:\n",
    "      Probability of observed sequence under the HMM model as a float.\n",
    "    \"\"\"\n",
    "    \n",
    "    seq_len = len(obs_seq)\n",
    "    num_states = transitions.shape[0]\n",
    "    \n",
    "    # Intialise the array to hold forward values for each state at each timestep\n",
    "    forward = np.zeros((seq_len, num_states))\n",
    "    \n",
    "    # Extract the emission probabilities for this particular obs_seq\n",
    "    emissions_seq = emissions[obs_seq]\n",
    "    \n",
    "    # Set first forward scores depending on initial distribution and emission probs\n",
    "    forward[0] = initials * emissions_seq[0]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Forward values at step 0: {forward[0]}\\n\")\n",
    "\n",
    "    for step in range(1, seq_len):\n",
    "        \n",
    "        # Calculate forward values for this step for all states using matrix multiplications\n",
    "        # The new forward values depend on forward values at previous step, transitions, and emissions\n",
    "        forward[step] = np.sum(forward[step-1].reshape(-1, 1) * transitions * emissions_seq[step], axis=0)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Forward values at step {step}: {forward[step]}\\n\")\n",
    "            \n",
    "    # Sum over forward values at last timestep\n",
    "    seq_prob = np.sum(forward[seq_len-1], axis=0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Total probability of observed sequence: {seq_prob:.4g}\")\n",
    "        \n",
    "    return seq_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd879c",
   "metadata": {},
   "source": [
    "Let's find the probability of a particular sequence of observations. We will run the forward algorithm with verbose output so we can track how the forward probabilities change throughout calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e6b8e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward values at step 0: [0.35 0.05]\n",
      "\n",
      "Forward values at step 1: [0.0295 0.0735]\n",
      "\n",
      "Forward values at step 2: [0.031955 0.005735]\n",
      "\n",
      "Forward values at step 3: [0.0054569 0.0020811]\n",
      "\n",
      "Forward values at step 4: [0.0034929  0.00025482]\n",
      "\n",
      "Total probability of observed sequence: 0.003748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0037477100000000005"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate sequence probability, printing out intermediate values\n",
    "obs_seq = np.array([2, 0, 2, 1, 2])\n",
    "forward(obs_seq, transitions, emissions, initials, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dff0e2",
   "metadata": {},
   "source": [
    "We are now able to solve the evaluation problem with our HMM. For any observation sequence, we can figure out how likely it is. Let's see the results for a few more sequences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c6ce7a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sequence [0, 0, 0, 0, 0]: 0.02398\n",
      "Probability of sequence [1, 1, 1, 1, 1]: 0.00032\n",
      "Probability of sequence [2, 2, 2, 2, 2]: 0.03962\n",
      "Probability of sequence [0, 1, 1, 2, 2]: 0.003718\n",
      "Probability of sequence [0, 1, 1, 2, 2, 2]: 0.002105\n"
     ]
    }
   ],
   "source": [
    "# Print the probabilities of several sequences, rounded to 4 sig figs\n",
    "\n",
    "print((f\"Probability of sequence {[0, 0, 0, 0, 0]}: \"\n",
    "       f\"{forward([0, 0, 0, 0, 0], transitions, emissions, initials):.4g}\")\n",
    ")\n",
    "\n",
    "print((f\"Probability of sequence {[1, 1, 1, 1, 1]}: \"\n",
    "       f\"{forward([1, 1, 1, 1, 1], transitions, emissions, initials):.4g}\")\n",
    ")\n",
    "\n",
    "print((f\"Probability of sequence {[2, 2, 2, 2, 2]}: \"\n",
    "       f\"{forward([2, 2, 2, 2, 2], transitions, emissions, initials):.4g}\")\n",
    ")\n",
    "\n",
    "print((f\"Probability of sequence {[0, 1, 1, 2, 2]}: \"\n",
    "       f\"{forward([0, 1, 1, 2, 2], transitions, emissions, initials):.4g}\")\n",
    ")\n",
    "\n",
    "print((f\"Probability of sequence {[0, 1, 1, 2, 2, 2]}: \"\n",
    "       f\"{forward([0, 1, 1, 2, 2, 2], transitions, emissions, initials):.4g}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31dd88",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Consider these outputs - can you (qualitatively) explain any of them? Some possible guiding questions include:\n",
    "\n",
    "- The emission probability has a kind of symmetry between the states. Given this, what can account for the difference in probabilities of the sequences [0, 0, 0, 0, 0] and [2, 2, 2, 2, 2]?\n",
    "\n",
    "- Looking at the emission probability matrix, can you explain why it is less likely to observe Jason eating only one ice cream every day compared to zero or two ice creams?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490a1e4",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "There are many ways to answer these questions, and the idea is that students ensure they understand how both the transition and emission probabilities affect the result. Possible solutions include:\n",
    "\n",
    "- The difference is due to the transition matrix. Ultimately, hot days are slightly more likely. Since  $P(2\\ ice\\ creams|Hot)$ and $P(0\\ ice\\ creams| Cold)$ are equal, along with similar symmetry for the other emission probabilities, the likelihood of hidden state sequences with many hot days is the factor leading to a higher sequence probability for [2, 2, 2, 2, 2].\n",
    "\n",
    "\n",
    "- No matter the hidden state (ie. for all sequences), the emission probabilities of observing consumption of one ice cream are relatively low (0.2). For zero or two ice creams, there is a slightly lower emission probability in one state [$P(0\\ ice\\ creams|Hot)=P(2\\ ice\\ creams| Cold)=0.1$], but a much higher emission probability in the other [$P(0\\ ice\\ creams|Cold)=P(2\\ ice\\ creams| Hot)=0.7$]. These latter emissions account for the much higher probabilities of [0, 0, 0, 0, 0] and [2, 2, 2, 2, 2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436aee8f",
   "metadata": {},
   "source": [
    "## 4. Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde5e0ea",
   "metadata": {},
   "source": [
    "Another fundamental HMM task is decoding. \n",
    "\n",
    "> Given an HMM and an observation sequence, what is the most likely sequence of hidden states?\n",
    "\n",
    "Tasks such as part-of-speech tagging fit into this framework, where the observations are words and the hidden state we would like to know is the part-of-speech labels (think nouns, verbs etc.).\n",
    "\n",
    "Recall from the lecture that this problem can be efficiently solved using the Viterbi program, a dynamic programming algorithm in similar spirit to the forward algorithm. \n",
    "\n",
    "It uses intermediate Viterbi values:\n",
    "$$V_{k}(i)=\\max _{\\pi_{1} \\cdots \\pi_{i-1}} P\\left(\\pi_{1}, \\cdots, \\pi_{i-1}, o_{1}, o_2, \\cdots o_{i}, \\pi_{i}=k\\right)$$ which are the probabilities of the most likely sequence of states ending at state $\\pi_i=k$.\n",
    "\n",
    "As for the forward algorithm let's recall the steps from the lecture.\n",
    "\n",
    "*Initialisation*: $V_{k}(0) = a_k * e_{\\pi_{k}}(o_0)$ \n",
    "\n",
    "*Iteration*: $$\n",
    "\\begin{array}{l}\n",
    "V_{j}(i)= \\max _{k} [a_{k j} * V_{k}(i-1) * e_{j}\\left(o_{i}\\right)]\\\\\n",
    "P t r_{j}(i)=\\underset{k}{\\operatorname{argmax}} [a_{k j} * V_{k}(i-1) * e_{j}\\left(o_{i}\\right)]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "*Termination*: $P(O, \\pi^*)=\\max _{k}V_{k}(M)$\n",
    "\n",
    "This is the joint probability of the observations and best hidden state sequence. To find the best sequence, we can backtrack through our pointer variables we stored when taking maximums for the Viterbi score computations at each timestep.\n",
    "\n",
    "Take a look at the implementation below and find each step of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f92c3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(obs_seq, transitions, emissions, initials):\n",
    "    \"\"\"Given HMM parameters, find the state path that maximises likelihood of an observed sequence.\n",
    "    \n",
    "    Args:\n",
    "      obs_seq: 1D numpy array containing a sequence of observations encoded as indices\n",
    "      transitions: 2D square numpy array of transition probabilities between states\n",
    "      emissions: 2D numpy array of shape (num_obs, num_states) containing emissions probabilities P(obs|state)\n",
    "      initials: 1D numpy array of initial distribution over states\n",
    "      \n",
    "    Returns:\n",
    "      Tuple of the state path found and associated Viterbi scores\n",
    "    \"\"\"\n",
    "    \n",
    "    seq_len = len(obs_seq)\n",
    "    num_states = transitions.shape[0]\n",
    "    \n",
    "    # Initialise required arrays to store viterbi values\n",
    "    path_states = np.zeros((seq_len, num_states))\n",
    "    path_scores = np.zeros((seq_len, num_states))\n",
    "    best_state_seq = np.zeros(seq_len, dtype=np.int64)\n",
    "    \n",
    "    # Extract the emission probabilities for this particular obs_seq\n",
    "    emissions_seq = emissions[obs_seq]\n",
    "    \n",
    "    # Set first forward scores depending on initial distribution and emission probs\n",
    "    path_scores[0] = initials * emissions_seq[0]\n",
    "    \n",
    "    for step in range(1, seq_len):\n",
    "\n",
    "        # Generate candidate Viterbi scores for each state using matrix summations\n",
    "        candidate_scores = path_scores[step-1].reshape(-1, 1)\\\n",
    "                           * transitions\\\n",
    "                           * emissions_seq[step]\n",
    "        \n",
    "        # Update Viterbi scores and paths by maximising over candidate scores\n",
    "        path_scores[step] = np.max(candidate_scores, axis=0)\n",
    "        path_states[step] = np.argmax(candidate_scores, axis=0)\n",
    "        \n",
    "    # Termination step: choose final state by max Viterbi score\n",
    "    best_state_seq[seq_len-1] = np.argmax(path_scores[seq_len-1], axis=0)\n",
    "    \n",
    "    # Backtrack over the best path found by Viterbi\n",
    "    for step in range(seq_len-1, 0, -1):\n",
    "        next_state = best_state_seq[step]\n",
    "        best_state_seq[step-1] = path_states[step, next_state]\n",
    "\n",
    "    return best_state_seq, path_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db9c43",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Let's observe the results on a particular sequence of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6533da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hot', 'Hot', 'Hot', 'Hot', 'Hot']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hot</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.01568</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.001405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cold</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.00343</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1        2         3         4\n",
       "Hot   0.35  0.028  0.01568  0.002509  0.001405\n",
       "Cold  0.05  0.049  0.00343  0.000627  0.000050"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_seq = np.array([2, 0, 2, 1, 2])\n",
    "\n",
    "# Use Viterbi to decode the observed sequence\n",
    "states_seq, state_prob = viterbi(obs_seq, transitions, emissions, initials)\n",
    "\n",
    "# Visualise the output\n",
    "df = pd.DataFrame(state_prob.transpose(), index=[\"Hot\", \"Cold\"])\n",
    "state_names = [\"Hot\", \"Cold\"]\n",
    "print([state_names[s] for s in states_seq])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5788d4",
   "metadata": {},
   "source": [
    "Note that despite the low probability of Jason eating zero ice creams on a hot day, the second day (index 1) is likely hot when taken in context with the rest of the sequence due to the relevant transition probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a374ed7",
   "metadata": {},
   "source": [
    "### A warning about implementing Viterbi\n",
    "One thing you may notice is that the Viterbi algorithm involves multiplication of lots of probability values, and the Viterbi scores quickly become very small. While this is not a problem for the sequences we deal with here, this could cause issues with floating point numbers for very long sequences. A practical trick when implementing the Viterbi algorithm is to do the computations in log space instead. Recalling the laws of logarithms, that means the multiplications become sums of logarithms instead: ie. $a * b \\rightarrow \\log(a*b) = \\log(a) + \\log(b)$. Since logarithms are montonic functions, all the maxes and argmaxes will work out the same! We can even recover the Viterbi scores in the original space by taking the exponent at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe6c0cc",
   "metadata": {},
   "source": [
    "## 5. Task - Detecting a Biased Die\n",
    "\n",
    "Suppose you have some inside information that a casino has been switching to a biased die during games. You wish to determine when they are using a fair die and when they are using the biased one to optimise your betting strategy.\n",
    "\n",
    "Your source has disclosed that the games always begin with a fair die. For every roll after that, there is a 1/10 chance that the current die will be switched to the other kind (ie. switch fair die to biased die, or vice versa).\n",
    "\n",
    "The fair die, as expected, has an equal 1/6 chance of rolling each number 1 to 6. The biased die rolls a 1 50% of the time, and each other number 2 through 6 comes up 10% of the time.\n",
    "\n",
    "This problem can be modelled using an HMM. Your first task is to define the appropriate matrices for the HMM.\n",
    "\n",
    "Armed with your HMM, suppose you observe the following sequence of 15 rolls at the casino:\n",
    "[1, 5, 6, 2, 4, 6, 1, 3, 6, 2, 1, 1, 1, 4, 6]\n",
    "\n",
    "- Determine the probability of this sequence of rolls given what we know about the possible use of a biased die. How does this compare to the sequence probability under a fair die ($(1/6)^n$)?\n",
    "\n",
    "\n",
    "- Determine when the fair die and biased die were most likely being used.\n",
    "\n",
    "Hint: remember that while the rolls are 1-6, our states are usually indexed from 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153eb55",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281d7eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = np.array([[0.9, 0.1],\n",
    "                        [0.1, 0.9]])\n",
    "\n",
    "initials = np.array([1, 0])\n",
    "\n",
    "emissions = np.array([[1./6, 0.5],\n",
    "                      [1./6, 0.1],\n",
    "                      [1./6, 0.1],\n",
    "                      [1./6, 0.1],\n",
    "                      [1./6, 0.1],\n",
    "                      [1./6, 0.1]])\n",
    "\n",
    "obs_seq = [1, 5, 6, 2, 4, 6, 1, 3, 6, 2, 1, 1, 1, 4, 6]\n",
    "\n",
    "# Subtract one from each roll to get the corresponding state index\n",
    "obs_seq = [roll-1 for roll in obs_seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f0d78",
   "metadata": {},
   "source": [
    "Calculate the probability of this sequence and compare the probability under fair die:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "288e958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability under HMM: 4.926e-12\n",
      "Probability under fair die: 2.127e-12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Probability under HMM: {forward(obs_seq, transitions, emissions, initials):.4g}\")\n",
    "\n",
    "print(f\"Probability under fair die: {(1/6)**15:.4g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba7242",
   "metadata": {},
   "source": [
    "Predict when the dice is fair and biased using Viterbi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e975e30d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fair', 'Fair', 'Fair', 'Fair', 'Fair', 'Fair', 'Fair', 'Fair', 'Fair', 'Fair', 'Biased', 'Biased', 'Biased', 'Biased', 'Fair']\n"
     ]
    }
   ],
   "source": [
    "dice_used, viterbi_scores = viterbi(obs_seq, transitions, emissions, initials)\n",
    "\n",
    "# Print list with state names\n",
    "print([\"Fair\" if x==0 else \"Biased\" for x in dice_used])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110f120a",
   "metadata": {},
   "source": [
    "Under the HMM, Viterbi predicts that the sequence of several ones in a row was due to the biased die!\n",
    "\n",
    "For your curiousity, the following code was used to generate this sequence. You might like to generate some other roll sequences by rerunning the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a05946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_die(seq_len=1):\n",
    "    \"\"\"Return a sequence of rolls from a fair or biased die as defined in problem description above.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: how many rolls to perform\n",
    "    \n",
    "    Returns: \n",
    "        A list of results from dice rolls and a list of which dice was used for each roll.\n",
    "    \"\"\"\n",
    "    \n",
    "    rolls = []\n",
    "    dice_used = []\n",
    "    \n",
    "    # Initially the dice is unbiased\n",
    "    biased = False\n",
    "\n",
    "    for roll_num in range(seq_len):\n",
    "        \n",
    "        if biased:\n",
    "            # Roll biased die\n",
    "            roll = np.random.choice([1, 2, 3, 4, 5, 6], p=[0.5, 0.1, 0.1, 0.1, 0.1, 0.1])\n",
    "        else:\n",
    "            # Roll fair die\n",
    "            roll = np.random.choice([1, 2, 3, 4, 5, 6], p=[1./6 for i in range(6)])\n",
    "\n",
    "        rolls.append(roll)\n",
    "        dice_used.append(\"Biased\") if biased else dice_used.append(\"Fair\")\n",
    "        \n",
    "        # Change to other die with 1/10 probability\n",
    "        if np.random.random() < 0.1:\n",
    "            biased = not biased\n",
    "\n",
    "    return rolls, dice_used\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4a353",
   "metadata": {},
   "source": [
    "### Application of this problem to bioinformatics\n",
    "While they may seem simple, biased dice or fair casino problems are directly related to an application of HMMs in bioinformatics. \n",
    "\n",
    "In DNA, a cystine (C) followed by a guanine (G) is called a CpG dinuleotide. These dinucleotides are less frequent than expected by random chance, except for in specific regions called CpG islands ([Wikipedia](https://en.wikipedia.org/wiki/CpG_site#CpG_islands)), where they are clustered. These regions are important to find because they are often associated with the start of a gene. \n",
    "\n",
    "Given a genomic sequence, this detection problem can be modelled similarly to the biased dice problem, where the states are {non-island, CpG island} and the observations are the dinucleotides in the DNA. We can decode to determine when the sequence is in a CpG island. \n",
    "\n",
    "[Wu et al. (2010)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2883304/) have utilised HMMs on this problem, and a more thorough explanation can be found [here](http://www.math.clemson.edu/~macaule/classes/f16_math4500/slides/f16_math4500_cpg-islands_handout.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcdb16f",
   "metadata": {},
   "source": [
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4375b6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of HMM matrices\n",
    "transitions = np.array([[0.8, 0.2],\n",
    "                        [0.3, 0.7]])\n",
    "\n",
    "initials = np.array([0.5, 0.5])\n",
    "\n",
    "emissions = np.array([[0.1, 0.7],\n",
    "                      [0.2, 0.2],\n",
    "                      [0.7, 0.1]])\n",
    "\n",
    "# Solve the evaluation/likelihood problem using the forward algorithm\n",
    "seq_prob = forward(obs_seq, transitions, emissions, initials, verbose=True)\n",
    "\n",
    "# Solve the decoding problem using the Viterbi algorithm\n",
    "states_seq, viterbi_scores = viterbi(obs_seq, transitions, emissions, initials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e2bcb",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "- The ice cream example was adapted from:\n",
    "\n",
    "*Eisner, J. (2002, July). An interactive spreadsheet for teaching the forward-backward algorithm. In Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics (pp. 10-18). Link: https://www.cs.jhu.edu/~jason/papers/eisner.tnlp02.pdf*\n",
    "\n",
    "- The biased coin/die problem is widespread in HMM sources. The Fair Bet Casino problem in the following book was consulted:\n",
    "\n",
    "*Neil C. Jones & Pavel A. Pevzner 2004, An Introduction to Bioinformatics Algorithms, Computational Molecular Biology, The MIT Press, Cambridge, MA*\n",
    "\n",
    "- For the information on HMMs in bioinformatics:\n",
    "\n",
    "*Wu, H., Caffo, B., Jaffee, H. A., Irizarry, R. A., & Feinberg, A. P. (2010). Redefining CpG islands using hidden Markov models. Biostatistics (Oxford, England), 11(3), 499–514. https://doi.org/10.1093/biostatistics/kxq005*\n",
    "\n",
    "http://www.math.clemson.edu/~macaule/classes/f16_math4500/slides/f16_math4500_cpg-islands_handout.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
