{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5318 Week 4: Naive Bayes. Model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy import signal\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#for accuracy_score, classification_report and confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will firstly learn how to create a Naive Bayes (NB) classifier in sklearn. Then we will learn how to obtain different performance metrics (precision, recall, F1-score and confusion matrix), how to apply different procedures for evaluating the performance of classifiers (cross-validation and leave-one-out) and finally, how to use grid search with cross-validation for model selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating an NB classifier\n",
    "\n",
    "There are four main types of NB classifiers in sklearn:  <b>GaussianNB</b>, <b>CategoricalNB</b>,  <b>MultinomialNB</b> and <b>BernoulliNB</b>. \n",
    "\n",
    "- The first two are the ones we discussed at the lecture -  <b>GaussianNB</b> is applicable to numeric data, while <b>CategoricalNB</b> is applicable to categorical data. \n",
    "\n",
    "- <b>BernoulliNB</b> and <b>MultinomialNB</b> are mostly used for text clasification; they assume binary data and count data respectively, e.g. how many times a word appears in a document.\n",
    "\n",
    "In this tutorial we will create a NB for the iris data which is a numeric dataset, so we will use the <b>GaussianNB</b> class to create the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the iris data and create the training and test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# create the training and test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, stratify=iris.target, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "Use GaussianNB from sklearn.naive_bayes to create an NB classifier on the training data and evaluate its acuracy on the test data.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.921\n"
     ]
    }
   ],
   "source": [
    "# Solution:\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# create an NB model using the training set and evaluate its accuracy on the test set\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "print(\"Accuracy on test set: {:.3f}\".format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. More performance measures: precision, recall and F1 score. Confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to accuracy, we can calculate other performance measures - e.g. precision, recall and their combination - the F1-score. In sklearn this can be convenintly done using the <b>classification_report</b> method, which also shows the accuracy.\n",
    "\n",
    "### Task:\n",
    "\n",
    "1) Continuing on the previous exercise (NB classifier on the iris data), write the Python code to calculate precision, recall, F1 measure and confusion matrix on the test set by using the methods classification_report and confusion_matrix.\n",
    "\n",
    "See:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "2) Examine the results:\n",
    "-- How are the precision, recall and F1-score calculated - per class or overall for the test set?\n",
    "-- Making sense of the confusion matrix: Where are the correctly classified examples? How many examples from class 1 are incorrectly classified as class 2?\n",
    "\n",
    "\n",
    "### Solution:\n",
    "1) See the code below\n",
    "\n",
    "2) Precision, recall and F1-score are calculated per class  and the average is also shown (two different version - weighted by the number of examples and not weighted).  Recall that there are 3 classes for the iris data:\n",
    "class 0 - setosa, class 1 - versicolor and class 2 - virginica.\n",
    "\n",
    "The confusion matrix is a 3-by-3 array, where the rows correspond to the true classes and the columns correspond to the predicted class. Thus, in the confusion matrix below (shown at the end in []):\n",
    "- 1 example from class 1 was incorrectly classified as class 2\n",
    "- 2 examples from class 2 were incorectly classified as as class 1\n",
    "- the correctly classified examples are along the main diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        12\n",
      "           1       0.86      0.92      0.89        13\n",
      "           2       0.92      0.85      0.88        13\n",
      "\n",
      "    accuracy                           0.92        38\n",
      "   macro avg       0.92      0.92      0.92        38\n",
      "weighted avg       0.92      0.92      0.92        38\n",
      "\n",
      "[[12  0  0]\n",
      " [ 0 12  1]\n",
      " [ 0  2 11]]\n"
     ]
    }
   ],
   "source": [
    "# calculate other performance measures - F1, precision and recall \n",
    "#print(nb)\n",
    "actual = y_test\n",
    "predicted = nb.predict(X_test)\n",
    "print(metrics.classification_report(actual, predicted))\n",
    "\n",
    "# show the confusion matrix\n",
    "print(metrics.confusion_matrix(actual, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-validation for evaluating performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation, in particular <b>10-fold stratified cross-validation</b>, is the standard method in machine learning for evaluating the performance of classification and prediction models. Recall that we are interested in the generalization performance, i.e. how well a classifier will perform on new, previously unseen data.\n",
    "\n",
    "To perform cross-validation in sklearn, we can use the <b>cross_val_score</b> function. It takes as parameters the classifier we would like to evaluate and the data - the feature vectors and the target classes (also caled ground-truth labels). The parameter <b>cv</b> specifies the number of folds; the default value is 3, so we need to set it to 10 for 10-fold cross-validation. Note that this function performs <b>stratified</b> cross validation for classification tasks. \n",
    "\n",
    "Let's evaluate our NB classifier using 10-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.93333333 0.93333333 1.         0.93333333 0.93333333 0.93333333\n",
      " 0.86666667 1.         1.         1.        ]\n",
      "Average cross-validation score: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(nb, iris.data, iris.target, cv=10)\n",
    "print(\"Cross-validation scores: {}\".format(scores)) #accuracy for each fold\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean())) #average accuracy over all folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important result is the average cross-validation score (the average accuracy over the the 10 folds) but it is also useful to look at the accuracy for each fold. In our case, we can see that there is a relatively high variation between the 10 folds  - from 86% to 100% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: \n",
    "Compare NB's cross-validation accuracy with NB's accuracy when we used a single training/test split. How can you explain the difference? Which is the more reliable measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "- Training/test split - this involves a single data split into training and test set; a classifier is build once, on the training set, and evaluated on the test set\n",
    "- 10-fold cross validation - this involves building 10 different classifiers, each time using a diferent training and test set(see the lecture notes about how this is done), and then calculating the average accuracy over the 10 runs\n",
    "\n",
    "Cross validation is a better evaluation procedure than a single training/test evaluation; it gives a more reliable accuracy estimate for the performance on new data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave-one-out is a special case of cross-validation where each fold is a single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluations:  150\n",
      "Mean accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "one_out = LeaveOneOut()\n",
    "scores = cross_val_score(nb, iris.data, iris.target, cv=one_out)\n",
    "print(\"Number of evaluations: \", len(scores))\n",
    "print(\"Mean accuracy: {:.2f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "What are the advantages and disadvantages of leave-one-out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "- Advantage: Provides better estimate of the generalization accuracy.\n",
    "- Disadvantage: Slow, especially for large datasets (number of evaluations = number of examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Grid search with cross-validation for parameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can improve the generalization performance of machine learning algorithms by tuning their parameters. NB doesn't have many parameters but in the previous weeks we saw that the performance of k-Nearest Neighbor algorithm depends on the number of neighbors and distance measure type, the performance of regression models depends on the values of <b>alpha</b> and <b>C</b>. \n",
    "\n",
    "In sklearn we can use grid search with cross-validation to search through different parameter combinations and select the best one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider k-nearest neighbor (k-NN) as an example and tune two of its parameters by considering the following values:\n",
    "- number of neareast neighbours <b>n_neighbours</b> = 1, 3, 5, 11 and 13\n",
    "- distance measure - Manhattan and Euclidean, which can be controlled by the value of parameter <b>p</b>, 1 or 2 respectively\n",
    "\n",
    "This gives us 5 x 2 combinations of paramneter values. We would like to find the best combination - the one that we expect to generalise well on new examples.\n",
    "\n",
    "We will use the following procedure, called <b>grid-search with cross-validation for parameter tuning</b>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Create the parameter grid (i.e. the parameter combinations)\n",
    "Split the data into training set and test set\n",
    "For each parameter combination\n",
    "    Train a k-NN classifier on the training data using 10-fold cross-validation as an evaluation procedure\n",
    "    Compute the cross-validation accuracy cv_acc\n",
    "    If cv_acc > best_cv_acc \n",
    "       best_cv_acc = cv_acc\n",
    "       best_parameters = current parameters\n",
    "Rebuild the k-NN model using the whole training data and best_parameters\n",
    "Evaluate it on the test data and report the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data is split into training set and test set\n",
    "- The cross-validation loop uses the training data. It is performed for every parameter combination. Its purpose is to select the best parameter combination - the one with the highest cross-validation accuracy. This involves, for every parameter combination, building 10 models on 90% of the training data (9 folds) and evaluating them on the remaining 10% (1 fold).\n",
    "- Once this is done, a new model is trained using the selected best parameter combination on the <b>whole training set</b> and evaluated on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid:\n",
      "{'n_neighbors': [1, 3, 5, 11, 15], 'p': [1, 2]}\n",
      "Test set score: 0.95\n",
      "Best parameters: {'n_neighbors': 11, 'p': 1}\n",
      "Best cross-validation score: 0.98\n",
      "Best estimator:\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=11, p=1,\n",
      "                     weights='uniform')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irena\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': [1, 3, 5, 11, 15],\n",
    "              'p': [1, 2]}\n",
    "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10,\n",
    "                          return_train_score=True)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "print(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: We create an object of type GridSearchCV, which is then fitted to the <b>training</b> data. This fitting includes 2 things: \n",
    "1. Searching for and determining the best parameter combination - the one with the best cross-validation accuracy <b>and</b> \n",
    "2. Building a new model on the whole training set with the best parameter combination from 1.\n",
    "\n",
    "It is important to understand the difference between <b>best cross-validation score</b> and <b>test set score </b>:\n",
    "- <b>best cross-validation score</b> is the mean cross-validation accuracy, with cross-validation performed on the <b>training set</b>. This step involves building 10 models on the training data, each time using 9 folds together (90% of the training data) to create the model and testing this model the remaining 10th fold (10% of the training data). The purpose of this step is to select the best parameter combination, which is the one with the highest cross-validation accuracy.\n",
    "- <b>test set score</b> - this is the the accuracy on the test of a model that was created using the <b>whole training set</b> (100%) with the selected parameters. This is the result that we report as a measure of generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB().fit(X_train, y_train)\n",
    "scores = cross_val_score(nb, iris.data, iris.target, cv=10)\n",
    "scores = cross_val_score(nb, iris.data, iris.target, cv=one_out)\n",
    "\n",
    "param_grid = {'n_neighbors': [1, 3, 5, 11, 15], 'p': [1, 2]}\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=10, return_train_score=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "This tutorial is based on:\n",
    "\n",
    "Aurelien Geron (2019). Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow, O'Reilly.\n",
    "\n",
    "Andreas C. Mueller and Sarah Guido (2016). Introduction to Machine Learning with Python: A Guide for Data Scientists, O'Reilly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
